{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Salamander Siamese Network\n",
        "\n",
        "Created by Alejandro Marin (816035363)\n",
        "\n",
        "Edited by:"
      ],
      "metadata": {
        "id": "ActRCViQycQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thought Process:\n",
        "\n",
        "We initially thought that this was a simple classification problem. However, when we observed the sample solution we realised that this was way more complicated. We needed to actually identify the specific animal by an ID. So our first thought was a Siamese network where we compare the test image with the database.\n"
      ],
      "metadata": {
        "id": "N4Hm3i8izFMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKOQ1kqopOOY"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow pandas matplotlib opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTJmsUxyp7B4",
        "outputId": "fcd00d8b-32db-4e56-8950-bd1470627417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'SalamanderID2025.zip', 'metadata.csv', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q SalamanderID2025.zip -d ."
      ],
      "metadata": {
        "id": "UIq_2V2WIGuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the original metadata but remove images\n",
        "df = pd.read_csv('metadata.csv')\n",
        "\n",
        "# Fixed the 'path' column to remove 'images/' prefix - using colab and loading each separately, image is WAY too big to load\n",
        "df['path'] = df['path'].str.replace(r'^images/', '', regex=True)\n",
        "\n",
        "# Save the fixed version for reuse in later phases\n",
        "df.to_csv('metadata_fixed.csv', index=False)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZWVutThxvBX",
        "outputId": "a6a9e0ee-4281-4a20-debe-9aa8f6a05004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   image_id            identity  \\\n",
            "0         0  LynxID2025_lynx_37   \n",
            "1         1  LynxID2025_lynx_37   \n",
            "2         2  LynxID2025_lynx_49   \n",
            "3         3                 NaN   \n",
            "4         4  LynxID2025_lynx_13   \n",
            "\n",
            "                                                path date orientation species  \\\n",
            "0  LynxID2025/database/000f9ee1aad063a4485379ec06...  NaN       right    lynx   \n",
            "1  LynxID2025/database/0020edb6689e9f78462394d5a6...  NaN        left    lynx   \n",
            "2  LynxID2025/database/003152e4145b5b6940091d5c12...  NaN        left    lynx   \n",
            "3  LynxID2025/query/003b89301c7b9f6d18f722082617f...  NaN        back    lynx   \n",
            "4  LynxID2025/database/003c3f82011e9c3f849f945a93...  NaN       right    lynx   \n",
            "\n",
            "      split     dataset  \n",
            "0  database  LynxID2025  \n",
            "1  database  LynxID2025  \n",
            "2  database  LynxID2025  \n",
            "3     query  LynxID2025  \n",
            "4  database  LynxID2025  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load new metadata\n",
        "df = pd.read_csv('metadata_fixed.csv')\n",
        "\n",
        "# Step 1: Filter by only SeaTurtleID2022 + database images (so the ones that specifically labelled as database - refer to metadata csv)\n",
        "salamander_df = df[(df['dataset'] == 'SalamanderID2025') & (df['split'] == 'database')]\n",
        "\n",
        "# Step 2: Drop rows with missing values (shouldn't though, just precaution)\n",
        "salamander_df = salamander_df.dropna(subset=['identity', 'orientation', 'path'])\n",
        "\n",
        "# Step 3: Group images by identity and orientation - now this is because like I expolained before we are going to have different phases and will deal with the orientation in later phases, first is to ensure is works\n",
        "grouped_data = defaultdict(lambda: defaultdict(list)) #this would be used, the lambda part is to initialise any missing data as an empty defaultdict - this is to store all the\n",
        "#data\n",
        "\n",
        "for _, row in salamander_df.iterrows():\n",
        "    identity = row['identity']\n",
        "    orientation = row['orientation'].lower()\n",
        "    path = row['path']\n",
        "\n",
        "    grouped_data[identity][orientation].append(path)     #now recap if forgotten but we would be taking the direct images in the database folder and use the info from metadata to get\n",
        "    #the actual label of this image, this is why we do this and have it in this format\n",
        "\n",
        "#Check the first two\n",
        "for identity, orientations in list(grouped_data.items())[:2]:\n",
        "    print(f\"Identity: {identity}\")\n",
        "    for orientation, paths in orientations.items():\n",
        "        print(f\"  Orientation: {orientation}, {len(paths)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqtDcqxKZWQ3",
        "outputId": "871d9f11-8764-416e-e73e-4e51dda867c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identity: SalamanderID2025_2\n",
            "  Orientation: top, 4 images\n",
            "Identity: SalamanderID2025_3\n",
            "  Orientation: top, 3 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import combinations\n",
        "\n",
        "positive_pairs = [] #for phase 1 I want to deal with same orientation and same ID - to make sure it works (no val yet)\n",
        "\n",
        "for identity, orientation_dict in grouped_data.items():\n",
        "    for orientation, paths in orientation_dict.items():\n",
        "\n",
        "        if len(paths) >= 2:# Ensures there are at least two image paths in the paths  list for the current orientation and ID\n",
        "            for img1, img2 in combinations(paths, 2):  #combinations (python itertools)\n",
        "                positive_pairs.append((img1, img2, 1))  # 1 = this would be positive label\n",
        "\n",
        "# Shuffle for training randomness\n",
        "random.shuffle(positive_pairs)\n",
        "\n",
        "print(f\"Generated {len(positive_pairs)} positive pairs\")\n",
        "print(\"Sample:\", positive_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVV2zxOCdtBw",
        "outputId": "8aca47d7-95b3-435d-a619-d8eb4e9e76f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1126 positive pairs\n",
            "Sample: [('SalamanderID2025/database/images/80a618b159b58d6c_1043.jpg', 'SalamanderID2025/database/images/a5f79371654ddfe4_1343.jpg', 1), ('SalamanderID2025/database/images/145b2e3b47f30c2a_172.jpg', 'SalamanderID2025/database/images/ec52f4e22dccdb1d_1903.jpg', 1), ('SalamanderID2025/database/images/1478452802817df8_174.jpg', 'SalamanderID2025/database/images/e605136528466ad5_1863.jpg', 1), ('SalamanderID2025/database/images/b124fc14367bf188_1441.jpg', 'SalamanderID2025/database/images/2fd5149df1ddee16_386.jpg', 1), ('SalamanderID2025/database/images/9f2b0f1d1cd6d838_1290.jpg', 'SalamanderID2025/database/images/866528982db541b3_1093.jpg', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "def preprocess_image(path):  #preprocess for NN as normal\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "5SeG26uaeBIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the triplets into three lists\n",
        "img1_paths = [pair[0] for pair in positive_pairs]\n",
        "img2_paths = [pair[1] for pair in positive_pairs]\n",
        "labels     = [pair[2] for pair in positive_pairs]\n"
      ],
      "metadata": {
        "id": "ZmOhWCfLeEqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels)) #sets up a tf dataset pipeline to process image pairs\n",
        "#convert each input into a dataset (each item a tuple)\n",
        "\n",
        "def load_pair(img1_path, img2_path, label):\n",
        "    img1 = preprocess_image(img1_path) #prep each image\n",
        "    img2 = preprocess_image(img2_path)\n",
        "    return (img1, img2), tf.cast(label, tf.float32) #float 32 is standard for  tf ops\n",
        "\n",
        "dataset = dataset.map(load_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "3crSvdtQeHOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1.1 Training - Same Orientation (Same ID)"
      ],
      "metadata": {
        "id": "cKT6a579e3mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Siamese Model (Base CNN)"
      ],
      "metadata": {
        "id": "sLmSne-bfHQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Shared CNN base -  a normal tf CNN\n",
        "def create_embedding_network(input_shape=(224, 224, 3)):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs) #conv layer with 64 filters, 3x3 kernel\n",
        "    x = layers.MaxPooling2D()(x)  #maxpool to \"downsize\" - reduce but keep info by highest value in block\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dense(128)(x)  # final embedding vector\n",
        "\n",
        "    return Model(inputs, x, name=\"EmbeddingNetwork\")\n"
      ],
      "metadata": {
        "id": "P1-6o1fGfAEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(y_true, y_pred, margin=0.5):\n",
        "    # y_true: 1 if same, 0 if different\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "class DistanceLayer(tf.keras.layers.Layer):\n",
        "    def call(self, anchor, candidate):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(anchor - candidate + 1e-9), axis=1))\n"
      ],
      "metadata": {
        "id": "2clqE-cefVXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct SN"
      ],
      "metadata": {
        "id": "d2DeJi5MfeFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape (img1, img2)\n",
        "input_shape = (224, 224, 3)\n",
        "embedding_net = create_embedding_network(input_shape)\n",
        "\n",
        "# Two input branches\n",
        "input_a = tf.keras.Input(shape=input_shape)\n",
        "input_b = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "# Pass both through the same embedding network\n",
        "embedding_a = embedding_net(input_a)\n",
        "embedding_b = embedding_net(input_b)\n",
        "\n",
        "# Distance calculation\n",
        "distance = DistanceLayer()(embedding_a, embedding_b)\n",
        "\n",
        "# Full model\n",
        "siamese_model = tf.keras.Model(inputs=[input_a, input_b], outputs=distance)\n",
        "siamese_model.compile(optimizer='adam', loss=contrastive_loss)\n"
      ],
      "metadata": {
        "id": "KHz1fpS7fcT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit"
      ],
      "metadata": {
        "id": "cnFcTl4ffkyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with the dataset built earlier\n",
        "siamese_model.fit(dataset, epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHnqiB5vfj5y",
        "outputId": "c880cc1a-2921-4116-cba5-0cd0b5b454b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 290ms/step - loss: 0.1017\n",
            "Epoch 2/3\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 186ms/step - loss: 2.0500e-08\n",
            "Epoch 3/3\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 186ms/step - loss: 6.0786e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7849a35ad450>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1.2 Training - Same Orientation (with Negative pairs)"
      ],
      "metadata": {
        "id": "14482RQ04fgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import product\n",
        "\n",
        "# Phase 1.2: Generate negative pairs (different identity, same orientation)\n",
        "negative_pairs = []\n",
        "\n",
        "identities = list(grouped_data.keys())\n",
        "\n",
        "for i in range(len(identities)):\n",
        "    id1 = identities[i]\n",
        "    for j in range(i + 1, len(identities)):\n",
        "        id2 = identities[j]\n",
        "\n",
        "        # Find shared orientations\n",
        "        common_orientations = set(grouped_data[id1].keys()) & set(grouped_data[id2].keys())\n",
        "\n",
        "        for orientation in common_orientations:\n",
        "            paths1 = grouped_data[id1][orientation]\n",
        "            paths2 = grouped_data[id2][orientation]\n",
        "\n",
        "            # Create all cross-identity pairs for the shared orientation\n",
        "            for img1, img2 in product(paths1, paths2):\n",
        "                negative_pairs.append((img1, img2, 0))  # 0 = negative\n",
        "\n",
        "print(f\"Generated {len(negative_pairs)} negative pairs\")\n",
        "print(\"Sample:\", negative_pairs[:5])\n",
        "\n",
        "\n",
        "positive_pairs = []\n",
        "\n",
        "for identity, orientation_dict in grouped_data.items():\n",
        "    for orientation, paths in orientation_dict.items():\n",
        "        # Generate all unique combinations of two images for same ID & orientation\n",
        "        if len(paths) >= 2:\n",
        "            for img1, img2 in combinations(paths, 2):\n",
        "                positive_pairs.append((img1, img2, 1))  # 1 = positive label\n",
        "\n",
        "# Shoofle\n",
        "random.shuffle(positive_pairs)\n",
        "\n",
        "print(f\"Generated {len(positive_pairs)} positive pairs\")\n",
        "print(\"Sample:\", positive_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdLt_gme5ENI",
        "outputId": "b3833c22-6430-4250-fe87-2a1eb66d3722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 485513 negative pairs\n",
            "Sample: [('SalamanderID2025/database/images/eafd41b675ff3330_1893.jpg', 'SalamanderID2025/database/images/56d75a5db470298e_703.jpg', 0), ('SalamanderID2025/database/images/eafd41b675ff3330_1893.jpg', 'SalamanderID2025/database/images/5c80da39ea556a4f_749.jpg', 0), ('SalamanderID2025/database/images/eafd41b675ff3330_1893.jpg', 'SalamanderID2025/database/images/e254f8215312b187_1832.jpg', 0), ('SalamanderID2025/database/images/e80dc66902b38838_1881.jpg', 'SalamanderID2025/database/images/56d75a5db470298e_703.jpg', 0), ('SalamanderID2025/database/images/e80dc66902b38838_1881.jpg', 'SalamanderID2025/database/images/5c80da39ea556a4f_749.jpg', 0)]\n",
            "Generated 1126 positive pairs\n",
            "Sample: [('SalamanderID2025/database/images/b01ad4ebc731652f_1428.jpg', 'SalamanderID2025/database/images/0a8470c0e83644dc_83.jpg', 1), ('SalamanderID2025/database/images/d6db420cedad7c63_1740.jpg', 'SalamanderID2025/database/images/0bf4548387fc2ad4_100.jpg', 1), ('SalamanderID2025/database/images/1478452802817df8_174.jpg', 'SalamanderID2025/database/images/0c971a6147192f42_105.jpg', 1), ('SalamanderID2025/database/images/5702a94da09ac5a4_705.jpg', 'SalamanderID2025/database/images/79abffe26a389170_993.jpg', 1), ('SalamanderID2025/database/images/8ee32686351de59b_1162.jpg', 'SalamanderID2025/database/images/7ee7919bb9333158_1027.jpg', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "\n",
        "# Preprocess\n",
        "def preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    return image / 255.0\n",
        "\n",
        "#load a pair of images and a label\n",
        "def load_pair(img1_path, img2_path, label):\n",
        "    img1 = preprocess_image(img1_path)\n",
        "    img2 = preprocess_image(img2_path)\n",
        "    return (img1, img2), tf.cast(label, tf.float32)\n",
        "\n",
        "# Build dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels))\n",
        "dataset = dataset.map(load_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "AndD3LDp5Nj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap to N pairs total (balanced between positive & negative)\n",
        "N = 10000  # NOTE - Adjust as needed\n",
        "\n",
        "# Shuffle first to ensure mix\n",
        "random.shuffle(positive_pairs)\n",
        "random.shuffle(negative_pairs)\n",
        "\n",
        "# Trim each (AGAIN, adjust N if needed but running this raw was pain)\n",
        "positive_pairs = positive_pairs[:N//2]\n",
        "negative_pairs = negative_pairs[:N//2]\n",
        "\n",
        "# Combine and shuffle again\n",
        "all_pairs = positive_pairs + negative_pairs\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "print(f\"Using {len(all_pairs)} total pairs for training.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjLlYIqe7zdu",
        "outputId": "c5ce07fe-e4d8-4498-9f76-cec4c8eba088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 6126 total pairs for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_pairs = positive_pairs + negative_pairs\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "img1_paths = [p[0] for p in all_pairs]\n",
        "img2_paths = [p[1] for p in all_pairs]\n",
        "labels     = [p[2] for p in all_pairs]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels))\n",
        "dataset = dataset.map(load_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "siamese_model.fit(dataset, epochs=3)    #SWITCH BACK TO 10 LATER\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7kwiZV16qzr",
        "outputId": "f6f62d12-ca91-40b3-dae1-1f58e484e6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 219ms/step - loss: 0.0674\n",
            "Epoch 2/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 192ms/step - loss: 0.0378\n",
            "Epoch 3/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 194ms/step - loss: 0.0291\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7849202a9cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1.3 Training - Same Orientation (Positive pairs have random augmentation)"
      ],
      "metadata": {
        "id": "BoeS6Bjp96lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "# Augmentation function\n",
        "def augment_image(image):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
        "    return image\n",
        "\n",
        "# Preprocess image with optional augmentation\n",
        "def preprocess_image(path, augment=False):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = image / 255.0\n",
        "    if augment:\n",
        "        image = augment_image(image)\n",
        "    return image\n",
        "\n",
        "# Load a pair with augmentation flag\n",
        "def load_augmented_pair(img1_path, img2_path, label, augment_flag):\n",
        "    img1 = preprocess_image(img1_path, augment=False)         # anchor\n",
        "    img2 = preprocess_image(img2_path, augment=augment_flag)  # candidate (possibly augmented)\n",
        "    return (img1, img2), tf.cast(label, tf.float32)\n",
        "\n",
        "# Phase 1.3: Mix positive + negative with some augmented\n",
        "augment_probability = 0.5\n",
        "augmented_positive_pairs = []\n",
        "\n",
        "for img1, img2, label in positive_pairs:\n",
        "    do_augment = random.random() < augment_probability\n",
        "    augmented_positive_pairs.append((img1, img2, 1, do_augment))\n",
        "\n",
        "# Negative pairs (no augment)\n",
        "augmented_negative_pairs = [(img1, img2, 0, False) for img1, img2, _ in negative_pairs]\n",
        "\n",
        "# Combine & shuffle\n",
        "combined_pairs = augmented_positive_pairs + augmented_negative_pairs\n",
        "random.shuffle(combined_pairs)\n",
        "print(f\"Total training pairs: {len(combined_pairs)}\")\n",
        "print(f\"Sample: {combined_pairs[:2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBA40vvR-WKC",
        "outputId": "e64def6c-b614-4d6c-b890-6f55adc671d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training pairs: 6126\n",
            "Sample: [('SalamanderID2025/database/images/360d3d2b98199920_442.jpg', 'SalamanderID2025/database/images/9aa9c32b0b7482cb_1262.jpg', 0, False), ('SalamanderID2025/database/images/928dd6b849070bd8_1189.jpg', 'SalamanderID2025/database/images/d25afdd14c607cbd_1702.jpg', 0, False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract columns\n",
        "img1_paths     = tf.constant([p[0] for p in combined_pairs])\n",
        "img2_paths     = tf.constant([p[1] for p in combined_pairs])\n",
        "labels         = tf.constant([p[2] for p in combined_pairs], dtype=tf.float32)\n",
        "augment_flags  = tf.constant([p[3] for p in combined_pairs], dtype=tf.bool)\n",
        "\n",
        "# Build TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels, augment_flags))\n",
        "dataset = dataset.map(load_augmented_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "7PgPsQjQ-Zcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.fit(dataset, epochs=3)  #CHANGE BACK TO 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5SF3fW8-fEB",
        "outputId": "b9a46fde-480b-4622-dbfd-36be1cca6bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 191ms/step - loss: 0.0216\n",
            "Epoch 2/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 192ms/step - loss: 0.0162\n",
            "Epoch 3/3\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 194ms/step - loss: 0.0121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7849af576210>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1.4 Training - Same Orientation (Add Test Split)"
      ],
      "metadata": {
        "id": "vSZs7oxOCbou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combined_pairs is already shuffled because we are reusing\n",
        "train_pairs, val_pairs = train_test_split(combined_pairs, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PleAWmEWCiuR",
        "outputId": "aab14e49-3020-4ccf-b3b4-87e1668d3e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs: 4900\n",
            "Validation pairs: 1226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(pairs, augment_on_img2=True):\n",
        "    img1_paths = tf.constant([p[0] for p in pairs])\n",
        "    img2_paths = tf.constant([p[1] for p in pairs])\n",
        "    labels     = tf.constant([p[2] for p in pairs], dtype=tf.float32)\n",
        "    augment_flags = tf.constant([p[3] if augment_on_img2 else False for p in pairs], dtype=tf.bool)\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((img1_paths, img2_paths, labels, augment_flags))\n",
        "    ds = ds.map(load_augmented_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.shuffle(1024).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_dataset = build_dataset(train_pairs)\n",
        "val_dataset   = build_dataset(val_pairs, augment_on_img2=False)  # Don't augment during validation\n"
      ],
      "metadata": {
        "id": "lTJejU6qC9f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.fit(train_dataset, validation_data=val_dataset, epochs=3)  #CHANGE BACK TO 10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqJ2VQHQDBUx",
        "outputId": "7d9ac252-3557-408c-b1d9-b77fe5a09bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 280ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 2/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 228ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 3/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 231ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 4/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 230ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 5/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 230ms/step - loss: 0.0018 - val_loss: 0.0021\n",
            "Epoch 6/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 230ms/step - loss: 0.0017 - val_loss: 0.0019\n",
            "Epoch 7/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 231ms/step - loss: 0.0019 - val_loss: 0.0022\n",
            "Epoch 8/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 229ms/step - loss: 0.0017 - val_loss: 0.0023\n",
            "Epoch 9/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 231ms/step - loss: 0.0014 - val_loss: 0.0021\n",
            "Epoch 10/10\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 231ms/step - loss: 0.0013 - val_loss: 0.0020\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a448c4777d0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2.1 Training - Change Orientations"
      ],
      "metadata": {
        "id": "vfa7Z8IxQCFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Generate positive pairs where the identity is the same but orientation is different\n",
        "cross_orientation_positive_pairs = []\n",
        "\n",
        "for identity, orientation_dict in grouped_data.items():\n",
        "    orientations = list(orientation_dict.keys())\n",
        "\n",
        "    # Cross-orientation pairs: left-right, front-back, etc.\n",
        "    for i in range(len(orientations)):\n",
        "        for j in range(i + 1, len(orientations)):\n",
        "            orient1 = orientations[i]\n",
        "            orient2 = orientations[j]\n",
        "            paths1 = orientation_dict[orient1]\n",
        "            paths2 = orientation_dict[orient2]\n",
        "\n",
        "            for img1, img2 in product(paths1, paths2):\n",
        "                cross_orientation_positive_pairs.append((img1, img2, 1))\n",
        "\n",
        "print(f\"Generated {len(cross_orientation_positive_pairs)} cross-orientation positive pairs\")\n",
        "print(\"Sample:\", cross_orientation_positive_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PervcvURSjgJ",
        "outputId": "d03b7d05-217e-484f-808a-15409c8fb6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1057 cross-orientation positive pairs\n",
            "Sample: [('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/300f05747c664509_388.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/db9ea4061b99894e_1781.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/dd28b3f3c330b764_1793.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/b1fb0538cda86891_1452.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/ccdec086fcbadc9f_1665.jpg', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuse old negative pair (will add orientated negative pair in 2.2)"
      ],
      "metadata": {
        "id": "BTLTJwfhUOSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine with existing negative pairs\n",
        "combined_pairs_2_1 = cross_orientation_positive_pairs + negative_pairs\n",
        "random.shuffle(combined_pairs_2_1)\n",
        "\n",
        "# Augmentation flags (only for positive, same logic as before)\n",
        "augment_probability = 0.5\n",
        "augmented_pairs_2_1 = []\n",
        "for img1, img2, label in combined_pairs_2_1:\n",
        "    do_augment = label == 1 and random.random() < augment_probability\n",
        "    augmented_pairs_2_1.append((img1, img2, label, do_augment))\n"
      ],
      "metadata": {
        "id": "MJzGRFLfUNvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split\n",
        "train_pairs_2_1, val_pairs_2_1 = train_test_split(augmented_pairs_2_1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset builder (reuses existing functions)\n",
        "train_dataset_2_1 = build_dataset(train_pairs_2_1)\n",
        "val_dataset_2_1   = build_dataset(val_pairs_2_1, augment_on_img2=False)\n",
        "\n",
        "# Train\n",
        "siamese_model.fit(train_dataset_2_1, validation_data=val_dataset_2_1, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-WRCr3TUZf1",
        "outputId": "cfdb1a9a-0b39-4c7c-a800-a6c53e2392c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 275ms/step - loss: 0.0450 - val_loss: 0.0405\n",
            "Epoch 2/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 224ms/step - loss: 0.0374 - val_loss: 0.0409\n",
            "Epoch 3/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 224ms/step - loss: 0.0232 - val_loss: 0.0311\n",
            "Epoch 4/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 225ms/step - loss: 0.0161 - val_loss: 0.0293\n",
            "Epoch 5/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 228ms/step - loss: 0.0123 - val_loss: 0.0279\n",
            "Epoch 6/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 228ms/step - loss: 0.0104 - val_loss: 0.0268\n",
            "Epoch 7/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 227ms/step - loss: 0.0080 - val_loss: 0.0252\n",
            "Epoch 8/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 227ms/step - loss: 0.0069 - val_loss: 0.0246\n",
            "Epoch 9/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 228ms/step - loss: 0.0055 - val_loss: 0.0246\n",
            "Epoch 10/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 229ms/step - loss: 0.0045 - val_loss: 0.0230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a44d44e04d0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2.2 Training - Change Orientation (Add Negative Orientation Change)"
      ],
      "metadata": {
        "id": "goPlwlXjXl1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Generate positive pairs where the identity is the same but orientation is different\n",
        "cross_orientation_positive_pairs = []\n",
        "\n",
        "for identity, orientation_dict in grouped_data.items():\n",
        "    orientations = list(orientation_dict.keys())\n",
        "\n",
        "    # Cross-orientation pairs: left-right, front-back, etc.\n",
        "    for i in range(len(orientations)):\n",
        "        for j in range(i + 1, len(orientations)):\n",
        "            orient1 = orientations[i]\n",
        "            orient2 = orientations[j]\n",
        "            paths1 = orientation_dict[orient1]\n",
        "            paths2 = orientation_dict[orient2]\n",
        "\n",
        "            for img1, img2 in product(paths1, paths2):\n",
        "                cross_orientation_positive_pairs.append((img1, img2, 1))\n",
        "\n",
        "print(f\"Generated {len(cross_orientation_positive_pairs)} cross-orientation positive pairs\")\n",
        "print(\"Sample:\", cross_orientation_positive_pairs[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WADHPHgMXz9B",
        "outputId": "f103f576-80bb-4d5b-8c25-5e914b4f23e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1057 cross-orientation positive pairs\n",
            "Sample: [('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/300f05747c664509_388.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/db9ea4061b99894e_1781.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/dd28b3f3c330b764_1793.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/b1fb0538cda86891_1452.jpg', 1), ('SalamanderID2025/database/images/30c4a661e585e5a7_392.jpg', 'SalamanderID2025/database/images/ccdec086fcbadc9f_1665.jpg', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_pairs = []\n",
        "\n",
        "identities = list(grouped_data.keys())\n",
        "\n",
        "for i in range(len(identities)):\n",
        "    id1 = identities[i]\n",
        "    for j in range(i + 1, len(identities)):\n",
        "        id2 = identities[j]\n",
        "\n",
        "        for ori1 in grouped_data[id1]:\n",
        "            for ori2 in grouped_data[id2]:\n",
        "                paths1 = grouped_data[id1][ori1]\n",
        "                paths2 = grouped_data[id2][ori2]\n",
        "\n",
        "                for img1, img2 in product(paths1, paths2):\n",
        "                    negative_pairs.append((img1, img2, 0))\n"
      ],
      "metadata": {
        "id": "INvSgmFGa4_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cap to N pairs total (balanced between positive & negative)\n",
        "N = 10000\n",
        "\n",
        "# Shuffle first to ensure mix\n",
        "random.shuffle(cross_orientation_positive_pairs)\n",
        "random.shuffle(negative_pairs)\n",
        "\n",
        "# Trim each\n",
        "cross_orientation_positive_pairs = cross_orientation_positive_pairs[:N//2]\n",
        "negative_pairs = negative_pairs[:N//2]\n",
        "\n",
        "# Combine with existing negative pairs\n",
        "combined_pairs_2_1 = cross_orientation_positive_pairs + negative_pairs\n",
        "random.shuffle(combined_pairs_2_1)\n",
        "\n",
        "print(f\"Using {len(combined_pairs_2_1)} total pairs for training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v_hJcilcONm",
        "outputId": "12fb7d2c-dede-4e88-a69a-8e3441cef714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 6057 total pairs for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Augmentation flags (only for positive, same logic as before)\n",
        "augment_probability = 0.5\n",
        "augmented_pairs_2_1 = []\n",
        "for img1, img2, label in combined_pairs_2_1:\n",
        "    do_augment = label == 1 and random.random() < augment_probability\n",
        "    augmented_pairs_2_1.append((img1, img2, label, do_augment))\n"
      ],
      "metadata": {
        "id": "8BFdzv-RbndY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split\n",
        "train_pairs_2_1, val_pairs_2_1 = train_test_split(augmented_pairs_2_1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset builder (reuses existing functions)\n",
        "train_dataset_2_1 = build_dataset(train_pairs_2_1)\n",
        "val_dataset_2_1   = build_dataset(val_pairs_2_1, augment_on_img2=False)\n",
        "\n",
        "# Train\n",
        "siamese_model.fit(train_dataset_2_1, validation_data=val_dataset_2_1, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l27KSVJYbqW_",
        "outputId": "9acad1cc-d0ee-4d52-a2ed-3e8d178b09ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 309ms/step - loss: 0.0364 - val_loss: 0.0397\n",
            "Epoch 2/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 234ms/step - loss: 0.0274 - val_loss: 0.0335\n",
            "Epoch 3/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 237ms/step - loss: 0.0178 - val_loss: 0.0314\n",
            "Epoch 4/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 236ms/step - loss: 0.0123 - val_loss: 0.0298\n",
            "Epoch 5/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 236ms/step - loss: 0.0103 - val_loss: 0.0286\n",
            "Epoch 6/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 237ms/step - loss: 0.0085 - val_loss: 0.0266\n",
            "Epoch 7/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 238ms/step - loss: 0.0072 - val_loss: 0.0261\n",
            "Epoch 8/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 239ms/step - loss: 0.0057 - val_loss: 0.0248\n",
            "Epoch 9/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 237ms/step - loss: 0.0049 - val_loss: 0.0246\n",
            "Epoch 10/10\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 237ms/step - loss: 0.0048 - val_loss: 0.0236\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7849805ad750>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Phase (Hopefully)"
      ],
      "metadata": {
        "id": "WF57BCYkeKo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First save model in case\n"
      ],
      "metadata": {
        "id": "8Tv3KGoceYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.save('siamese_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pWZfmToeR0Q",
        "outputId": "1aab2366-4f66-4c4d-c2e5-66fb488f00ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vY2s3tsRjqp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok now the fun begins (ps it is 6:00  am rn, started at 8 yesterday)\n"
      ],
      "metadata": {
        "id": "3f7eTPlTfWLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok I go sleep, continue later God willing. Have game exam tomorrow so maybe not idk"
      ],
      "metadata": {
        "id": "wCUl1K8yjnnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model.save('siamese_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "hGHRozUXj3M8",
        "outputId": "d7adcf15-905f-4bc1-c952-72f1ad75d785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=siamese_model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0948f0e7c523>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msiamese_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'siamese_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n\u001b[0;32m--> 114\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;34m\"Invalid filepath extension for saving. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;34m\"Please add either a `.keras` extension for the native Keras \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=siamese_model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "siamese_model = tf.keras.models.load_model('siamese_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "gNkDR0jKgx_i",
        "outputId": "4f2b8f1d-f419-42cd-f576-ee691b2e695f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown layer: 'DistanceLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-4cacc971f382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msiamese_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'siamese_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    194\u001b[0m         )\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;31m# Legacy format deserialization (no \"module\" key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;31m# used for H5 and SavedModel formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             layer = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# In this case we are dealing with a Keras config dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         (cls, cls_config) = class_and_config_for_serialized_keras_object(\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintable_module_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[0;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    352\u001b[0m     )\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0;34mf\"Unknown {printable_module_name}: '{class_name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;34m\"Please ensure you are using a `keras.utils.custom_object_scope` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown layer: 'DistanceLayer'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load metadata from scratch\n",
        "df = pd.read_csv('metadata_fixed.csv')\n",
        "\n",
        "# Step 1: Filter only SalamanderID2025 + query images\n",
        "query_df = df[(df['dataset'] == 'SalamanderID2025') & (df['split'] == 'query')]\n",
        "\n",
        "# Step 2: Drop rows with missing critical fields\n",
        "query_df = query_df.dropna(subset=['orientation', 'path'])\n",
        "\n",
        "# Step 3: Group by orientation only (no identity in query set)\n",
        "grouped_query = defaultdict(list)\n",
        "\n",
        "for _, row in query_df.iterrows():\n",
        "    orientation = row['orientation'].lower()\n",
        "    path = row['path']\n",
        "\n",
        "    grouped_query[orientation].append(path)\n",
        "\n",
        "# Check first few orientations and paths\n",
        "for orientation, paths in list(grouped_query.items())[:3]:\n",
        "    print(f\"Orientation: {orientation} → {len(paths)} images\")\n",
        "    print(\"  Sample:\", paths[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym24zpTUqLcO",
        "outputId": "c5c7e535-9945-4f93-eaa7-f0b16e599560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orientation: top → 391 images\n",
            "  Sample: ['SalamanderID2025/query/images/0737a5a022dc4a70_52.jpg', 'SalamanderID2025/query/images/421034673af2bedf_531.jpg']\n",
            "Orientation: left → 2 images\n",
            "  Sample: ['SalamanderID2025/query/images/0ca1b405a8494159_106.jpg', 'SalamanderID2025/query/images/07910c22b2a7de3c_57.jpg']\n",
            "Orientation: right → 296 images\n",
            "  Sample: ['SalamanderID2025/query/images/62a6adc09c8be8dc_806.jpg', 'SalamanderID2025/query/images/a3ad62a6f23aac6b_1329.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "THRESHOLD = 0.2  # Tune based on validation, so maybe look at a negative pair contrastive diff and use to set\n",
        "\n",
        "# Load database embeddings again if needed\n",
        "db_df = df[(df['dataset'] == 'SalamanderID2025') & (df['split'] == 'database')].dropna(subset=['identity', 'path'])\n",
        "\n",
        "db_image_paths = db_df['path'].tolist()\n",
        "db_identities = db_df['identity'].tolist()\n",
        "\n",
        "# Generate DB embeddings\n",
        "db_embeddings = []\n",
        "valid_db_paths = []\n",
        "\n",
        "for path in db_image_paths:\n",
        "    try:\n",
        "        img = preprocess_image(path)\n",
        "        img = tf.expand_dims(img, axis=0)\n",
        "        emb = embedding_net(img).numpy().squeeze()\n",
        "        db_embeddings.append(emb)\n",
        "        valid_db_paths.append(path)\n",
        "    except:\n",
        "        print(\"Skipping:\", path)\n",
        "\n",
        "db_embeddings = np.array(db_embeddings)\n",
        "print(f\"Loaded {len(db_embeddings)} DB embeddings\")\n",
        "\n",
        "# Run predictions for query images\n",
        "query_predictions = {}\n",
        "\n",
        "for orientation, query_paths in grouped_query.items():\n",
        "    for query_path in query_paths:\n",
        "        try:\n",
        "            img = preprocess_image(query_path)\n",
        "            img = tf.expand_dims(img, axis=0)\n",
        "            query_emb = embedding_net(img).numpy().squeeze()\n",
        "\n",
        "            # Compute Euclidean distances\n",
        "            dists = np.linalg.norm(db_embeddings - query_emb, axis=1)\n",
        "            min_idx = np.argmin(dists)\n",
        "            min_dist = dists[min_idx]\n",
        "\n",
        "            if min_dist > THRESHOLD:\n",
        "                predicted_id = \"unknown\"\n",
        "            else:\n",
        "                predicted_id = db_identities[min_idx]\n",
        "\n",
        "            query_predictions[query_path] = {\n",
        "                \"predicted_id\": predicted_id,\n",
        "                \"distance\": float(min_dist)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {query_path}:\", e)\n",
        "\n",
        "# Preview sample predictions\n",
        "for k, v in list(query_predictions.items())[:5]:\n",
        "    print(f\"Image Path: {k} -> {v['predicted_id']} (distance: {v['distance']:.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfPo3R3Rw8NT",
        "outputId": "e83fda33-060d-49d3-801f-55166768eef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1388 DB embeddings\n",
            "Image Path: SalamanderID2025/query/images/0737a5a022dc4a70_52.jpg -> SalamanderID2025_438 (distance: 0.1652)\n",
            "Image Path: SalamanderID2025/query/images/421034673af2bedf_531.jpg -> SalamanderID2025_212 (distance: 0.1768)\n",
            "Image Path: SalamanderID2025/query/images/4534bd79f3e6b736_561.jpg -> SalamanderID2025_347 (distance: 0.1821)\n",
            "Image Path: SalamanderID2025/query/images/6ba9cca71738e4ed_870.jpg -> SalamanderID2025_157 (distance: 0.1424)\n",
            "Image Path: SalamanderID2025/query/images/79449d8ed0ff0262_989.jpg -> SalamanderID2025_345 (distance: 0.1900)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Pick 2 different identities\n",
        "unique_ids = list(grouped_data.keys())\n",
        "id1, id2 = random.sample(unique_ids, 2)\n",
        "\n",
        "# Randomly select one image from each identity\n",
        "img1_path = random.choice([img for imgs in grouped_data[id1].values() for img in imgs])\n",
        "img2_path = random.choice([img for imgs in grouped_data[id2].values() for img in imgs])\n",
        "\n",
        "print(f\"Testing with:\")\n",
        "print(f\"Image 1: {img1_path} (ID: {id1})\")\n",
        "print(f\"Image 2: {img2_path} (ID: {id2})\")\n",
        "\n",
        "# Preprocess and embed\n",
        "img1 = preprocess_image(img1_path)\n",
        "img2 = preprocess_image(img2_path)\n",
        "img1 = tf.expand_dims(img1, axis=0)\n",
        "img2 = tf.expand_dims(img2, axis=0)\n",
        "\n",
        "emb1 = embedding_net(img1)\n",
        "emb2 = embedding_net(img2)\n",
        "\n",
        "# Compute Euclidean distance (contrastive loss = dist)\n",
        "distance = tf.norm(emb1 - emb2).numpy().item()\n",
        "\n",
        "print(f\"Computed distance: {distance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2pPY71lz0kY",
        "outputId": "429be8f5-5607-4c2a-c1ce-c56705670e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with:\n",
            "Image 1: SalamanderID2025/database/images/2259c7546282471a_286.jpg (ID: SalamanderID2025_534)\n",
            "Image 2: SalamanderID2025/database/images/145b2e3b47f30c2a_172.jpg (ID: SalamanderID2025_406)\n",
            "Computed distance: 0.4615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export to csv"
      ],
      "metadata": {
        "id": "_KWPevRC6AD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"salamander_predictions.csv\", \"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"query_path\", \"predicted_id\", \"distance\"])\n",
        "    for path, pred in query_predictions.items():\n",
        "        writer.writerow([path, pred['predicted_id'], pred['distance']])\n"
      ],
      "metadata": {
        "id": "_ELwm1To58or"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}